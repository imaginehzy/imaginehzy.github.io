---
layout: post
title: "如何训练出一条阿法狗"
description: ""
category: articles
tags: [AI]
---



![AlphaGo](http://7xq85r.com1.z0.glb.clouddn.com/836ad67d62458c744c2253b87f14ea51.jpeg)

在我写这篇文章时，阿法狗对李世石第四盘，在李世石下出绝妙的一手后，计算机下出了极坏的回应。李世石终于给予人类希望扳会一局。昨天是Alpha Go对李世石的第三盘，在做饭间我看了开局，阿法狗又祭出了新招式，感觉李世石开局就下出坏棋。

于是没有接着看棋，我转而去看了DeepMind发表在Nature上面的论文**Mastering the Game of Go with Deep Neural Networks and Tree Search**，仔细的学习了下阿法狗是怎么训练，在实战是怎么样运作的。由于关于机器学习我只有皮毛的知识，也没用实践过，我的讲解应该是不难理解的。（也有可能有细节上的错误）

## 梗概


阿法狗是由这么四部分组成的：

1. 走子策略网络（policy network）
2. 快速走子（fast rollout）
3. 局面评估网络（value network）
4. 蒙特卡洛树搜索（monte-carlo tree search，MCTS）

上面这三部分，在赛前中进行数据训练得出来的**功能**，而第4部分，则是在实战中，将1-3的功能来进行计算的部分。先解释一下前三部分设置，蒙特卡洛树搜索留在最后部分讲。

**走子策略网络（policy network）** 负责给出在当前布局下，可以走的盘面选择的走子概率。对于己方，就是概率越高越推荐走的着法；对于敌方，就是猜测对方最后可能走子的敌方。**快速走子（fast rollout）** 和走子策略网络其实是做同样的功能，只不过它用的是传统的专家归纳的特征pattern和知识作为依据。这两方面的实现在后面一点讲。

**局面评估网络（value network）** 负责的是给出当前布局，己方是优势是劣势的判断。按照Facebook的阿法狗竞争项目DarkForest的负责人田渊栋的介绍，对于局面优劣的判断是由来已久围棋AI的难点，他们黑暗森林也没用做这个部分。以往的局面优劣判断，我估计就是点目+假设点棋子对周围有控制+形状厚薄判断之类的综合。而不是对局面以及未来走势完整的给出判断。在这次人机大战中，似乎AI的给出局面判断，在开始时是往往和常人感觉有点不一样的。

### 深度学习

由于我懂也只是皮毛，这部分我只能将我知道的简单说明一下。

据说从生理学上神经网络模型得到其实，人类发现多层的神经元网络具有 **学习**的能力，这里的“学习"的对象指的是能够以获得某种功能，譬如识别人脸，猫脸，识别数字，手写签名，识别借贷业务中没有能力还贷款的人。具有学习能力意味，在”学习“也就是训练后，准确率会获得提升。

![神经网络](http://7xq85r.com1.z0.glb.clouddn.com/da03415312964e76cef62170685a87c8.png)

我们来看看神经网络长什么样先。如上图，一个32*32像素的带数字3的图输入进来，经过第一层神经元节点，又变成28x28的子图，然后再经过第二层神经元，又进一步划分特征，这样经过几层后特征越划越细，然后进入第二阶段，通过特征的权重进入分类器，分类器又可能是几层，然后生成的结果是像是否一张人脸，是否一个数字，还有如所有合法下子的地方的概率，或者局面是否优劣这样一个或者多个结果。

接着我们举一个人脸识别的例子，这些在视觉识别上都差不多。

![视觉识别例子](http://7xq85r.com1.z0.glb.clouddn.com/cdd9ea4ba57d99d3c5db277fcec537ea.png)

![人脸识别](http://7xq85r.com1.z0.glb.clouddn.com/564ba0c236568.jpg)

人脸识别利用卷积神经网络的思想是这样的，人类要识别一张人脸，我们看眼睛，首先是发现眼珠子的点以及上下眼睑的边缘，鼻子是上到下两边渐宽的线。我们就是先从底层的特征开始识别，然后在更高层次将他们识别成眼睛和鼻子。然后眼睛和鼻子，嘴巴，耳朵等等，其他又组成更高的层次脸。神经网络识别人脸就是这么一个由低到高的抽象过程。

阿法狗的两张神经网络走子策略网络和局面评估网络就是类似结构的东西。因为一个棋盘是19x19，我们看可以简单看做是黑白空（1，-1，0）三种状态的图，而第一层应该就是19x19=361个输入。据说这两个网络都有30多层的深度。至于为什么这样设计？和应该设置多少层，这个的确没有什么绝对的标注，和最终正确的答案，就是一种类似“艺术”的东西，人类是不是这样我们真不知道，只是知道它是有效的，或者设计性能比较高这样子。


### 扩展阅读

这里有一篇的文章[卷积神经网络](http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/)

卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。非全连接的部分称为特征层，全连接的部分称为隐含层。非全连接的特征层里面可以共享相同的权重。

它的非全连接和权值共享的网络结构使之更类似于生物 神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。

### 训练

深度学习一定需要很多数据，那么数据从哪里来呢？主要来自以往的高手下棋数据以及网络对战平台KGS上的数据,一共是80万盘对局数据。在这里阿法狗的走子策略网络Policy Network首先用有监督学习（Supervising learning，SL）的方法训练出来的。所谓有监督学习就是有人类监督。人类怎么监督呢？就是人类专家将棋局数据上的每一步走访标注为好的着法和坏的着法，这两个方向的数据将对走子策略网络，有不同的训练方式，将对网络数据进行调整。

每一层网络，每一个神经元节点都有一个**权重**，学习算法就是**调节**这些权重，还有最后增补的**bias偏差值**，这样使得网络最后变得更准确。

### 快速走子

接着，我们来了解一下**快速走子**是怎么回事？其实以往所有的围棋AI用的走子策略都是用快速走子的方法，而不是用深度学习。而这次google训练出来的快速走子居然在KGS有3d的水平，这已经是很高的了。这可能是Alpha Go两个爹之一的Aja Huang黄士杰有关，他以前就在围棋AI上很有建树，他写过很有优秀的围棋软件也曾打败之前的霸主Zen.

简单说，快速走子就是对一个局部作判断和分析有什么特征，如这块棋有几口气，这里是否有一个劫，这里连边界多远离中心多元，这里是否有一些定型（固定走法），死活怎么做，能否双活之类的。就是围棋专家归纳的特征，然后快速走子对一步棋在知识库里面看是否有匹配规则，或者对这些特征值做线性回归，比较简单决定是否走一步棋。

![快速走子特征](http://7xq85r.com1.z0.glb.clouddn.com/e295cd588b08a30aa723e22bef5fd750.jpeg)
（这张图说明了快速走子选取的特征数）

由于围棋是非常复杂的游戏，而专家的知识是有限的，规则往往设置的太简单，而不能应对当前复杂的局面。所以以往的围棋游戏的瓶颈就在这里。

那么，快速走子作为“落后”的东西，为什么DeepMind还要训练一个出来呢？这是因为快速走子的运算速度比通过走子策略网络计算一个步骤快很多。快多少呢？快速走子可以3微秒算一步，而走子策略网络需要毫秒级。两者差了1000倍以上的速度。在后面的蒙特卡洛树搜索里面，快速走子有起到作用，在后面我们再说。


### 左右互搏出来的另一个分身

到现在，通过有监督学习SL，利用围棋高手的下棋数据我们训练出来了一张走子策略网络，我们叫他policy network SL。然好，DeepMind进了一步，他们在policy network SL的基础上，通过另一种学习方法**强化学习Reinforce Learning RL**，通过自己和自己下棋训练出了另一张走子策略网络policy network RL还有，从自己对弈的3千万局数据中训练出来了**局面评估网络（value network）** 

![RL和Value network](http://7xq85r.com1.z0.glb.clouddn.com/0506dfb2ed40f37b0785fa99a8688d41.png)

**强化学习Reinforce Learning RL** 我理解就是通过梯度下降法来不停的提高自己的能力。在policy network SL作为第一代policy network RL，开始和自己对弈，对弈训练后对自己参数进行调整。然后迭代出来第二代policy network RL。为了防止过度拟合，就是过度像自己，只往一个方向发展，之后的每一代policy network RL，都从网络池里挑选前面相隔比较早的隔代policy network RL对弈进行训练。

这样policy network RL在和欧洲冠军对弈时已经进行了3千万局了，在和李世石对弈前，都不知道这个数据变成怎么样了。

而据说policy network RL下棋的走法不像policy network SL那么多样，但是通过强化训练后policy network RL对policy network SL的胜率已经达80%了。

![错误率](http://7xq85r.com1.z0.glb.clouddn.com/50e1505b83f27318e215d3e0077518b3.png)

（我们在这张图里面看，进行多少步后，各种走法的准确率的方差）

### 局面评估网络

通过RL在和自己各代版本对殴提升自己的过程中，产生了三千万局棋，这大概花了几个月时间。而人类下了有记录的棋或许只有几百万，远远不够样本数。DeepMind从这三千万局棋里面，每一句选择其中间一局面为样本，和最后的胜负结果作为输入，训练出来了另一张网络局面评估网络，就是我们在开头提到的第三部分。

之所以，每一局只选取一步，是因为一局棋里面，前后的棋着有相关关系，因此只要一步，和最后胜负结局就够了。从上图看，只用价值网络进行对弈，它的判断准确率跟policy network SL差不多。

## 对弈进行时——蒙特卡洛树搜索

这里还要对最后一种算法做解释，就是**蒙特卡洛树搜索**。我们知道蒙特卡洛是世界三大赌场之一，这种方法是跟概率有关的。

### 蒙特卡洛

这里有一篇阮一峰的文章[蒙特卡洛方法入门](http://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html)

简单的一个实例：

![mc](http://7xq85r.com1.z0.glb.clouddn.com/bg2015072604.jpg)

如上图，我们知道一个正方形里面有一个贴四边的圆.我们现在不用割圆法求圆周率pi。我们随机生成一些点，一些落在圆里面，一些落在圆外面。通过数落在圆里面，和所有点的数量，即落在整个正方形里面的点数量，我们大约可以猜出圆和正方形的面积比，因为圆比正方形面积为pi * r * r比4r * r。我们可以大约的求出pi为多少。当落点约多越精确。

### 树搜索

我们通过统计知道，国际象棋大概每一步有35种走法（随着子越来越少，但是相应可以走的位置越来越多），一局棋大概有双方80步。这样通过暴力穷举一步棋后面的着法就有35种可能，这样一棵可能着法的树会从开局的35种选择发展到35^80。这颗树如下面的规模

![国象树](http://7xq85r.com1.z0.glb.clouddn.com/Screen%20Shot%202016-03-13%20at%20%E4%B8%8B%E5%8D%885.50.09.png)

而围棋则是每一步大概250步（除了打劫提子的情况，围棋的可以着法应该是越来越少的），双方一般要下150着决胜负。这个可能性达到了惊人的10^170,而我们知道宇宙的所有原子大概是10^80.

![围棋树](http://7xq85r.com1.z0.glb.clouddn.com/Screen%20Shot%202016-03-13%20at%20%E4%B8%8B%E5%8D%885.50.38.png)

可见，要通过穷举围棋的树来找到所有着法中最好一着是不可能的。这意味着我们必须剪掉一些分支。

### 蒙特卡洛树搜索

大概是2000年之后，好像是法国人先搞出一个叫CrazyStone的围棋程序，用的方法是将蒙特卡洛和树搜索结合起来。开始是用专家式走子策略的方法选出几步，然后每一分支用随机的着法模拟双方走下去的局面，这样分支树越来越大越来越深。因为树是有限的，这样最后统计最初选择的几个分支哪一支总的胜率高，作为最终选择。

当然随机是不准确的，改进的方法就是，先通过调节参数的方式训练好前面说快速走子的方法。然后通过快速走子选取几个可能的走法分支，每一种，然后往后进行树搜索，每一步用快速走子选取一些点。最后统计走了着的树后面的胜负。决出最开始走的那几种走法哪一种最好。

![阿法狗的MCTS](http://7xq85r.com1.z0.glb.clouddn.com/ee06612df0fc764666c92624a3bc4c13.png)

如上图，而阿法狗是这么做的：

首先让走子策略网络（policy network SL或者RL）给出下棋最大概率的几着，然后SL可能会给出未来几步后的分支，之后就是用快速走子进行蒙特卡洛树的搜索，同时也用局面评估网络value network对当前给一个判断。局面评估网络和快速走子的蒙特卡罗树搜索，按照分配两个合起来为一的权重，然后为这一步的值。然后对整个分支依步数合并为一个总值，然后看最初哪一步的分支值最高就是计算机在比赛现场算出来走子的一步了。

论文说SL的下棋的多样性比RL好，但是value network用RL训练出来的效果比SL好。

![各部分输出](http://7xq85r.com1.z0.glb.clouddn.com/Screen%20Shot%202016-03-20%20at%20%E4%B8%8A%E5%8D%888.29.39.png)

Figure 5: How AlphaGo (black, to play) selected its move in an informal game against Fan Hui. For each of the following statistics, the location of the maximum value is indicated by an orange circle. 

+ a Evaluation of all successors s′ of the root position s, using the value network vθ(s′); estimated winning percentages are shown for the top evaluations. 
+ b Action-values Q(s,a) for each edge (s, a) in the tree from root position s; averaged over value network evaluations only (λ = 0).
+ c Action-values Q(s, a), averaged over rollout evaluations only (λ = 1). 
+ d Move probabilities directly from the SL policy network, pσ(a|s); reported as a percentage (if above 0.1%). 
+ e Percentage frequency with which actions were selected from the root during simulations. 
+ f The principal variation (path with maximum visit count) from AlphaGo’s search tree. The moves are presented in a numbered sequence. 

AlphaGo selected the move indicated by the red circle; Fan Hui responded with the move indicated by the white square; in his post-game commentary he preferred the move (1) predicted by AlphaGo.

## 两个后续问题

### DeepMind说从0训练一个Beta Go要怎么做？

若果，只给规则，让Beta Go从0开始学围棋，就是不能用人类高手的棋谱作为训练起点了。只能拿用前面说的强化学习，但是一开始网络的参数只能是随机的，围棋空间又那么巨大，让贝塔狗发展到人类高手的水平可能就没有那么容易了。

我想或许可以这么做：有一种基因算法，我们可以随机生成一些初始网络的参数权重，然后让他们去博弈，选择胜率高的网络。然后让种子网络互相交配（两个网络的权重各继承一部分），然后种子网络每一代迭代还有变异（让参数发生稍微变化）。

最后通过人工育种，选择胜率高的贝塔狗，在种子狗中终于有可能与人类对战的时候，开始强化学习。

这样培养出来的狗，下出来的棋风一定很有趣的。就像前面说的，人类其实探索了围棋空间中很少的一部分。

### 李世石走出一手好手后，阿法狗为什么会在后续着法下的那么臭？

阿法狗在这四盘棋中，常常因为对局面自我感觉比较良好，出现一些人类看来是缓着臭着的怪棋。我觉得可能是局面评估出了误判。也有可能是走子网络给出的最好着法其实不是最好的着法。

我们要知道，在比赛的时候，走子策略网络，快速走子，局面评估网络这三部分都是固定的，就是给一定的输入，给一定的输出。然后在蒙特卡洛树搜索的时候，我没有深究在这展开的方式在实现中有没有随机的因素，有的话就是有微小的不准确，没有的话，就是结果也是机械的。

补：后来第四局记者招待会上deepmind负责人哈斯比说，李世石下出神之一手78着后，局面评估网络value network评估局面为胜率70%。在接连9着开始犯傻的时候才发现局面已经变了。可见局面评估网络还是有缺陷的，对局面理解是不完整的。要想起value network是由那三千万局policy network RL自我对弈来的数据，或许正是这种单一性产生的结果。譬如大部分局面比较接近，不是70%那么悬殊。

也有可能三千万局每一局选择一步走子后局面判断，而其中神之一手这种大逆转的棋的数据量太少了。





