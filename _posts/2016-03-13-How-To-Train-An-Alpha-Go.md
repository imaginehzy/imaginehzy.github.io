---
layout: post
title: "如何训练出一只阿法狗"
description: ""
category: articles
tags: [AI]
---



![AlphaGo](http://7xq85r.com1.z0.glb.clouddn.com/836ad67d62458c744c2253b87f14ea51.jpeg)

昨天是Alpha Go对李世石的第三盘，在做饭间我看了开局，阿法狗又祭出了新招式，感觉李世石要0：5接受阿法狗教育的命运不可阻挡。虽然我也没有看过柯洁的棋，作为人类世界冠军，年少轻狂这种词也不适合戴他头上，唯有希望Google能够安排他们也战一次。

没有看棋，我转而去看了DeepMind发表在Nature上面的论文**Mastering the Game of Go with Deep Neural Networks and Tree Search**，仔细的学习了下阿法狗是怎么训练，在实战是怎么样运作的。由于关于机器学习我只有皮毛的知识，也没用实践过，我的讲解应该是不难理解的。（也有可能有细节上的错误）

##梗概##

阿法狗是由这么四部分组成的：

1. 走子策略网络（policy network）
2. 快速走子（fast rollout）
3. 局面评估网络（value network）
4. 蒙特卡洛树搜索（monte-carlo tree search，MCTS）

上面这三部分，在赛前中进行数据训练得出来的**功能**，而第4部分，则是在实战中，将1-3的功能来进行计算的部分。先解释一下前三部分设置，蒙特卡洛树搜索留在最后部分讲。

**走子策略网络（policy network）** 负责给出在当前布局下，可以走的盘面选择的走子概率。对于己方，就是概率越高越推荐走的着法；对于敌方，就是猜测对方最后可能走子的敌方。**快速走子（fast rollout）** 和走子策略网络其实是做同样的功能，只不过它用的是传统的专家归纳的特征pattern和知识作为依据。这两方面的实现在后面一点讲。

**局面评估网络（value network）** 负责的是给出当前布局，己方是优势是劣势的判断。按照Facebook的阿法狗竞争项目DarkForest的负责人田渊栋的介绍，对于局面优劣的判断是由来已久围棋AI的难点，他们黑暗森林也没用做这个部分。以往的局面优劣判断，我估计就是点目+假设点棋子对周围有控制+形状厚薄判断之类的综合。而不是对局面以及未来走势完整的给出判断。在这次人机大战中，似乎AI的给出局面判断，在开始时是往往和常人感觉有点不一样的。

##深度学习##

由于我懂也只是皮毛，这部分我只能将我知道的简单说明一下。

据说从生理学上神经网络模型得到其实，人类发现多层的神经元网络具有 **学习**的能力，这里的“学习"的对象指的是能够以获得某种功能，譬如识别人脸，猫脸，识别数字，手写签名，识别借贷业务中没有能力还贷款的人。具有学习能力意味，在”学习“也就是训练后，准确率会获得提升。

![神经网络](http://7xq85r.com1.z0.glb.clouddn.com/da03415312964e76cef62170685a87c8.png)

我们来看看神经网络长什么样先。如上图，一个32*32像素的带数字3的图输入进来，经过第一层神经元节点，又变成28x28的子图，然后再经过第二层神经元，又进一步划分特征，这样经过几层后特征越划越细，然后进入第二阶段，通过特征的权重进入分类器，分类器又可能是几层，然后生成的结果是想所有合法下子的地方的概率，或者局面是否优劣这样一个或者多个结果。

接着我们举一个人脸识别的例子，这些在视觉识别上都差不多。

![视觉识别例子](http://7xq85r.com1.z0.glb.clouddn.com/cdd9ea4ba57d99d3c5db277fcec537ea.png)

![人脸识别](http://7xq85r.com1.z0.glb.clouddn.com/564ba0c236568.jpg)

假设是一张64x64像素的图，通常会将其二值化，就是变成黑白灰的灰度图。然后经过多层次抽象特征，然后建立好分类器，就是该图是否是人脸。得出一个结果。至于这套理论的有效性的论证我这里也不说明，因为我也不太记得。

阿法狗的两张神经网络走子策略网络和局面评估网络就是类似结构的东西。因为一个棋盘是19x19，我们看可以简单看做是黑白空（1，-1，0）三种状态的图，而第一层应该就是19x19=361个输入。据说这两个网络都有30多层的深度。至于为什么这样设计？和应该设置多少层，这个的确没有什么绝对的标注，和最终正确的答案，就是一种类似“艺术”的东西，人类是不是这样我们真不知道，只是知道它是有效的，或者设计性能比较高这样子。

##训练##

深度学习一定需要很多数据，那么数据从哪里来呢？主要来自以往的高手下棋数据以及网络对战平台KGS上的数据。在这里阿法狗的走子策略网络Policy Network首先用有监督学习（Supervising learning，SL）的方法训练出来的。所谓有监督学习就是有人类监督。人类怎么监督呢？就是人类专家将棋局数据上的每一步走访标注为好的着法和坏的着法，这两个方向的数据将对走子策略网络，有不同的训练方式，将对网络数据进行调整。

每一层网络，每一个神经元节点都有一个**权重**，学习算法就是**调节**这些权重，还有最后增补的**bias偏差值**，这样使得网络最后变得更准确。

##快速走子##

接着，我们来了解一下**快速走子**是怎么回事？其实以往所有的围棋AI用的走子策略都是用快速走子的方法，而不是用深度学习。而这次google训练出来的快速走子居然在KGS有3d的水平，这已经是很高的了。这可能是Alpha Go两个爹之一的Aja Huang黄士杰有关，他以前就在围棋AI上很有建树，他写过很有优秀的围棋软件也曾打败之前的霸主Zen.

![错误率](http://7xq85r.com1.z0.glb.clouddn.com/50e1505b83f27318e215d3e0077518b3.png)

简单说，快速走子就是对一个局部作判断和分析有什么特征，如这块棋有几口气，这里是否有一个劫，这里连边界多远离中心多元，这里是否有一些定型（固定走法），死活怎么做，能否双活之类的。就是围棋专家归纳的特征，然后快速走子对一步棋在知识库里面看是否有匹配规则，或者对这些特征值做线性回归，比较简单决定是否走一步棋。

由于围棋是非常复杂的游戏，而专家的知识是有限的，规则往往设置的太简单，而不能应对当前复杂的局面。所以以往的围棋游戏的瓶颈就在这里。

那么，快速走子作为“落后”的东西，为什么DeepMind还要训练一个出来呢？这是因为快速走子的运算速度比通过走子策略网络计算一个步骤快很多。快多少呢？快速走子可以3微秒算一步，而走子策略网络需要毫秒级。两者差了1000倍以上的速度。在后面的蒙特卡洛树搜索里面，快速走子有起到作用，在后面我们再说。











